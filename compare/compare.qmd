---
title: "Comparison of Catalogs"
format: 
  html: 
    code-fold: true
    html-math-method: katex
  pdf: default
  arxiv-pdf:
    keep-tex: true    
toc: true
toc-depth: 3
theme: minti
bibliography: ../My_Library.bib
execute: 
  echo: false
  eval: true
  warning: false
---

# The data

In this script we will compare 2 catalogs @kovlakasHeraklionExtragalacticCatalogue2021 and [@karachentsevUPDATEDNEARBYGALAXY2013, @karachentsevSTARFORMATIONPROPERTIES2013a]

-   The data have been joined based on their position in the sky (Ra, Dec).
    -   We assume that every galaxy within 2 arc seconds of the initial coordinates is the same galaxy.
-   We use TOPCAT to create two joins, an inner and an outer join
-   We will use the inner join for 1-1 comparisons
-   If we see that the data are similar we can use the outer join
-   For the comparison we keep the parameters names exactly they are given in the catalogs

```{python}
#| echo: false
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import r2_score
import astropy
from astropy.io import ascii
from astropy.coordinates import SkyCoord
from astropy.table import Table, join, QTable
import astropy.units as u
from astropy.visualization import quantity_support, hist
import scipy
#quantity_support()
from astropy.stats import sigma_clip, SigmaClip
from astropy.modeling import models, fitting
from scipy.stats import pearsonr
import seaborn as sns
import pandas as pd
import glob
import os
from tabulate import tabulate
from IPython.display import Markdown, display

plt.style.use('ggplot')
pd.set_option('display.float_format', lambda x: '%.f' % x)

dt = QTable(ascii.read("../tables/inner_join.ecsv"), masked=True)

inner = dt
outer = QTable(ascii.read("../tables/outer_join.ecsv"), masked=True)
hec_not_lcv = QTable(ascii.read("../tables/HEC_not_LCV_join.ecsv"), masked=True)
lcv_not_hec = QTable(ascii.read("../tables/LCV_not_HEC_join.ecsv"), masked=True)
 
 
lcv = QTable(ascii.read("../tables/final_table.ecsv"), masked=True)
hec = QTable(ascii.read("../tables/HECATE_LCV.ecsv"), masked=True)
```

The dataset we are going to use for the comparison (inner join) consists of `{python} len(dt)` galaxies and `{python} len(dt.colnames)` columns.

# Catalog Completeness

Checking for completeness in galaxy catalogs is essential to ensure that the data accurately represents the true population of galaxies. Incomplete catalogs can lead to biased results in statistical studies, such as the distribution of galaxy luminosity, mass, or star formation rates. Additionally, missing galaxies, especially those at faint magnitudes or large distances, can distort cosmological measurements and hinder our understanding of galaxy formation and evolution.

Completeness checks are crucial for addressing selection biases, identify gaps in the data and guide follow-up observations, ensuring that the catalog provides a reliable sample for scientific analysis. Without these checks, conclusions drawn from the data may be inaccurate or incomplete.

## Completeness of the LCV Catalog

The local volume selection has been made by taking into account galaxies with: 

1. Radial velocities of
$$
    V_{LG}<600 \text{ km}\cdot\text{m}^{-1}
$$ {#eq-lcv-vel} 

2. Distances of D\<11 Mpc

A simultaneous fulfillment of both conditions (1) and (2) is not required.

-   Completeness within a 10 Mpc radius is difficult to assess due to:
    -   Variability in galaxy properties (luminosity, size, surface brightness, gas content)
    -   Errors in distance measurements (Tully–Fisher method errors of \~20–25%), especially at the 10 Mpc boundary.
        -   Accurate distances are mainly known within \~5 Mpc.
    -   Non-Hubble motions (\~300 km/s) may make up half of the adopted velocity constraint @eq-lcv-vel
        -   Solution for our usage: only keep the galaxies inside a radius of $D=11 Mpc$
        -   HI sources in surveys with low angular resolution: "The presence around our Galaxy of hundreds of high-velocity clouds with low line-of-sight velocities and small W50 widths also provokes the inclusion of false “nearby” dwarf galaxies in the LV"@karachentsevUPDATEDNEARBYGALAXY2013
        -   photographic emulsion defect -\> exotic cases of galaxies -\> radial velocity of $+614 km s^{−1}$
-   Astro-Spam and Survey Errors:
    -   Automatic surveys produce false detections (e.g., stars misclassified as galaxies, high-velocity clouds mistaken for dwarfs).
-   Conditional Completeness Estimate:
    -   Galaxies brighter than $M_B^c = -11^m$ or with linear diameters larger than $A_{26} = 1.0$ kpc show (40–60)% completeness.
    -   "among the members of the Local Group (D \< 1 Mpc), only half of the galaxies have absolute magnitudes brighter than −11m. Consequently, more than half of the ultra-faint dwarf companions around normal galaxies, like the Sombrero galaxy (D = 9.3 Mpc), still remain outside our field of view."
-   Undetected Ultra-Faint Dwarfs:
    -   Many faint dwarf galaxies remain undetected beyond the Local Group. Estimated population of undetected dwarfs could be as large as $10^3–10^4$ within the LV.
-   Surface Brightness Distribution:
    -   Surface brightness remains consistent across distances, except for ultra-faint dwarfs ($SB <31 \text{mag}\cdot\text{arcsec}^{-2}$).
    -   Faintest dwarf galaxies are detectable only nearby due to their resolution into individual stars.
-   Luminosity-Size Relationship:
    -   Observations align with cosmological models predicting $L\sim A^3$ @navarroStructureColdDark1996, though deviations occur for extremely low surface brightness galaxies.
    -   "The deviation from it at the extremely low surface brightness end is due to a systematic overestimation of dwarf galaxy sizes, the brightness profiles of which lie entirely below the Holmberg isophote."

## Completeness of HECATE

The completeness of HECATE is difficult to assess due to: - Unknown selection function of HyperLEDA and selection effects from other cross-correlated catalogs. - Estimation based on comparing B-band luminosity distribution with the galaxy luminosity function (LF).

-   HECATE is:

    -   Complete down to $L_B\sim 10^{9.5} L_{B,\odot}$ for distances less than 33 Mpc.

    -   Complete down to $L_B\sim 10^{10} L_{B,\odot}$ for distances between 67 Mpc and 100 Mpc.

    -   Incomplete at distances greater than 167 Mpc, even for the brightest galaxies.

-   Completeness estimates based on B-band luminosity density:

    -   \>75% complete for distances less than 100 Mpc. $\sim 50\%$ complete at distances of $\sim170$ Mpc.

    -   Completeness exceeds 100% within 30 Mpc due to the overdensity of galaxies around the Milky Way.

-   Completeness in terms of stellar mass (M\*):

    -   Similar to B-band completeness when measured with $K_s$-band luminosity as a tracer for stellar mass.

    -   Overdensity at small distances and cut-off at large distances are observed.

-   Completeness in terms of star formation rate (SFR):

    -   \~50% complete between 30 and 150 Mpc.

    -   Lower SFR completeness due to limitations in WISE-based SFR estimates, which lack full sky coverage.

    -   Despite IRAS’s limited depth, it provides \>50% coverage for star-forming galaxies in the local neighborhood.

    -   HECATE's nonuniform SFR and stellar mass coverage, affecting the reliability of stellar population parameters.

In this section we will check the completeness

```{python}
#| output: asis
# Creating a dataframe with the lengths of each table
data = {
    "Table": ["Inner join", "Outer join", "LCV", "HECATE", "Unique galaxies in LCV", "Unique Galaxies in Hecate"],
    "Number of galaxies": [len(dt), len(outer), len(lcv), len(hec), len(lcv_not_hec), len(hec_not_lcv)]
}

df_lengths = pd.DataFrame(data)

# Pretty print the dataframe in markdown format
df_lengths_md = df_lengths.to_markdown(index=False)
print(df_lengths_md)

```

## Completeness of the Inner join

$$
\text{Completeness (X)}=\frac{\text{(Galaxies in Inner Join)}}{\text{(Galaxies in X)}}×100\%
$$

Completeness (HECATE)= `{python} round(len(dt)/len(hec) *100)` %

Completeness (LCV)= `{python} round(len(dt)/len(lcv) *100)` %

## Completeness in Outer join

$$
\text{Completeness (X)}=\frac{\text{(Galaxies in Outer Join form X)}}{\text{(Galaxies in X)}}×100\%
$$

Completeness (HECATE)= `{python} round(len(hec_not_lcv)/len(hec)*100)` %

Completeness (LCV)= `{python} round(len(lcv_not_hec)/len(lcv)*100)` %

Combined Completeness =$\frac{\text{Total galaxies in Outer}}{\text{Unique galaxies in HECATE + LCV}}$= `{python} round(len(outer)/(len(lcv_not_hec)+(len(hec_not_lcv))) * 100)` %

## Completeness of the Data

```{python}
def plot_completeness(dataset_choice="lcv", column_name="Dis", description="Distance", units=True, logx=False, logy=False, bins=0, keep_nan=False):
    """
    Plots histograms with KDE and displays a table of counts and percentages for each bin, 
    based on the choice of dataset (LCV or HEC).
    
    Parameters:
    - dataset_choice: A string indicating which dataset to use ("lcv" or "hec")
    - column_name: The column name in the Qtable to plot and analyze (default is "Dis")
    - description: What to show in the title and the axis (default "Distance")
    - units: Show units or not (default is True)
    - logx: logarithmic x-axis (default is False)
    - logy: logarithmic y-axis (default is False)
    - bins: Number of bins or custom bin edges (default is 'auto')
    - keep_nan: keep (True) or remove NaN's (default is True)
    """
    
    if dataset_choice == "lcv":
        main_df, unique_df, label = lcv, lcv_not_hec, "LCV"
    elif dataset_choice == "hec" and hec is not None and hec_not_lcv is not None:
        main_df, unique_df, label = hec, hec_not_lcv, "HECATE"
    else:
        raise ValueError("Invalid dataset choice. Please choose 'lcv' or 'hec' with valid datasets.")
    
    # Remove NaN values before plotting
    if not keep_nan:
        main_data_clean = main_df[~np.isnan(main_df[column_name].data)].data
        unique_ddata_clean = unique_df[~np.isnan(unique_df[column_name].data)]
    else:
        main_data_clean = main_df[column_name].data
        unique_data_clean = unique_df[column_name].data
    
    # Check if bins is an integer or array of bin edges
    if bins == 0:
        counts, bin_edges = np.histogram(main_data_clean, bins='auto')
    elif isinstance(bins, int):
        counts, bin_edges = np.histogram(main_data_clean, bins=bins)
    else:
        bin_edges = bins  # If bins is an array of bin edges
    
    # Plot the histograms
    sns.histplot(main_data_clean, bins=bin_edges, kde=True, label=label)
    sns.histplot(unique_data_clean, bins=bin_edges, kde=True, label=f"Unique for {label}")
    
    # Set logarithmic axes if requested
    if logx:
        plt.xscale('log')
    if logy:
        plt.yscale('log')

    # Set the title and labels
    plt.title(f"Completeness of {description} for {label}")
    if units and main_df[column_name].unit is not None:
        plt.xlabel(f"{description} [{main_df[column_name].unit}]")
    else:
        plt.xlabel(f"{description}")
    plt.ylabel("Number of galaxies")
    plt.legend()

    # Calculate histogram counts for each dataset
    main_counts, _ = np.histogram(main_data_clean, bins=bin_edges)
    unique_counts, _ = np.histogram(unique_data_clean, bins=bin_edges)

    # Calculate bin centers for annotation
    bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])

    # Annotate percentages on the bars
    for i, center in enumerate(bin_centers):
        if main_counts[i] > 0:
            unique_pct = (unique_counts[i] / main_counts[i]) * 100
            plt.text(center, unique_counts[i], f'{unique_pct:.1f}%', color="green", ha='center', va='bottom')
    
    # Show the plot
    plt.show()
    plt.close()


```

```{python}
def table_completeness(dataset_choice, column_name="Dis", description = "Distance", keep_nan = False):
    """
    Creates a table of counts and percentages for each bin based on the chosen dataset (LCV or HEC).
    
    Parameters:
    - dataset_choice: A string indicating which dataset to use ("lcv" or "hec")
    - column_name: The column name in the datasets to analyze (default is "Dis")
    - description: what to show in the title and the axis (default "Distance")
    """
    
    if dataset_choice == "lcv":
        main_df, unique_df, label = lcv, lcv_not_hec, "LCV"
    elif dataset_choice == "hec" and hec is not None and hec_not_lcv is not None:
        main_df, unique_df, label = hec, hec_not_lcv, "HEC"
    else:
        raise ValueError("Invalid dataset choice. Please choose 'lcv' or 'hec' with valid datasets.")

    # Remove NaN values before plotting
    if keep_nan == False:
        main_df = main_df[~np.isnan(main_df[column_name].data)]
        unique_df = unique_df[~np.isnan(unique_df[column_name].data)]
    else:
        main_df = main_df[column_name].data
        unique_df = unique_df[column_name].data
    counts, bin_edges = np.histogram(main_df[column_name].data, bins='auto')

    main_counts, _ = np.histogram(main_df[column_name].data, bins=bin_edges)
    unique_counts, _ = np.histogram(unique_df[column_name].data, bins=bin_edges)
    inner_counts, _ = np.histogram(inner[column_name].data, bins=bin_edges)

    bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])
    table_data = []

    for i in range(len(bin_centers)):
        if main_counts[i] > 0:
            unique_pct = (unique_counts[i] / main_counts[i]) * 100
            inner_pct = (inner_counts[i] / main_counts[i]) * 100
            table_data.append({
                "Bin Start": round(bin_edges[i]),
                "Bin End": round(bin_edges[i+1]),
                f"Unique %": f'{unique_pct:.0f}%',
                "Inner %": f'{inner_pct:.0f}%'
            })
    table_df = pd.DataFrame(table_data)
    return table_df
```

### Distance

```{python}
#| fig-cap: "Histograms showing the Distance Completeness of the Catalogs"
#| layout-ncol: 2
#| fig-subcap: 
#| - "HECATE"
#| - "LCV"
#| label: fig-dis-comp

#HECATE

main = hec["D"].data
unique = hec_not_lcv["D"].data
counts, bin_edges = np.histogram(main)

main_counts, _ = np.histogram(main, bins=bin_edges)
unique_counts, _ = np.histogram(unique, bins=bin_edges)

plt.subplot(2,1,1)
sns.histplot(main, bins = bin_edges, kde = True, label = "HECATE")
sns.histplot(unique, bins = bin_edges, kde = True, label = "Unique for HECATE")
plt.legend()
plt.title("Completenes of Distance for HECATE")
unique_pct = (unique_counts/main_counts)*100 
###
plt.subplot(2,1,2)
ax = sns.lineplot(x = bin_edges[1:], y = unique_pct)
ax.set(xlabel='xlabel', ylabel='ylabel', title='title')

#changing ylables ticks
plt.yticks(np.arange(min(unique_pct), max(unique_pct)+1, 5.0))
y_value=['{:,.0f}'.format(x) + '%' for x in ax.get_yticks()]
ax.set_yticklabels(y_value)
plt.ylabel("Completeness %")
plt.xlabel("Distance [{}]".format(hec["D"].unit))
plt.title("")
plt.show()

#LCV
main = lcv["Dis"].data
unique = lcv_not_hec["Dis"].data
counts, bin_edges = np.histogram(main)

main_counts, _ = np.histogram(main, bins=bin_edges)
unique_counts, _ = np.histogram(unique, bins=bin_edges)

plt.subplot(2,1,1)
sns.histplot(main, bins = bin_edges, kde = True, label = "LCV")
sns.histplot(unique, bins = bin_edges, kde = True, label = "Unique for LCV")
plt.legend()
plt.title("Completenes of Distance for LCV")
unique_pct = (unique_counts/main_counts)*100 
###
plt.subplot(2,1,2)
ax = sns.lineplot(x = bin_edges[1:], y = unique_pct)
ax.set(xlabel='xlabel', ylabel='ylabel', title='title')

#changing ylables ticks
plt.yticks(np.arange(min(unique_pct), max(unique_pct)+1, 5.0))
y_value=['{:,.0f}'.format(x) + '%' for x in ax.get_yticks()]
ax.set_yticklabels(y_value)

plt.ylabel("Completeness %")
plt.xlabel("Distance [{}]".format(lcv["Dis"].unit))
plt.title("")
plt.show()

plt.close()
```

```{python}
#| fig-cap: "Histograms showing the Type Completeness of the Catalogs"
#| layout-ncol: 2
#| fig-subcap: 
#| - "HECATE"
#| - "LCV"
#| label: fig-type-comp
main = hec["T"].data
main = main[~np.isnan(main)]
unique = hec_not_lcv["T"].data
unique = unique[~np.isnan(unique)]
counts, bin_edges = np.histogram(main, bins = 12)

main_counts, _ = np.histogram(main, bins=bin_edges)
unique_counts, _ = np.histogram(unique, bins=bin_edges)

plt.subplot(2,1,1)
sns.histplot(main, bins = bin_edges, kde = True, label = "HECATE")
sns.histplot(unique, bins = bin_edges, kde = True, label = "Unique for HECATE")
plt.legend()
plt.title("Completenes of Types for HECATE")
unique_pct = (unique_counts/main_counts)*100 
###
plt.subplot(2,1,2)
ax = sns.lineplot(x = bin_edges[1:], y = unique_pct)
ax.set(xlabel='xlabel', ylabel='ylabel', title='title')

#changing ylables ticks
plt.yticks(np.arange(min(unique_pct), max(unique_pct)+1, 5.0))
y_value=['{:,.0f}'.format(x) + '%' for x in ax.get_yticks()]
ax.set_yticklabels(y_value)
plt.ylabel("Completeness %")
plt.xlabel("Types of galaxies")
plt.title("")
plt.show()

#LCV
main = lcv["TType"].data
main = main[~np.isnan(main)]
unique = lcv_not_hec["TType"].data
unique = unique[~np.isnan(unique)]
counts, bin_edges = np.histogram(main)

main_counts, _ = np.histogram(main, bins=bin_edges)
unique_counts, _ = np.histogram(unique, bins=bin_edges)

plt.subplot(2,1,1)
sns.histplot(main, bins = bin_edges, kde = True, label = "LCV")
sns.histplot(unique, bins = bin_edges, kde = True, label = "Unique for LCV")
plt.legend()
plt.title("Completenes of Distance for LCV")
unique_pct = (unique_counts/main_counts)*100 

plt.subplot(2,1,2)
ax = sns.lineplot(x = bin_edges[1:] ,y = unique_pct)
ax.set(xlabel='xlabel', ylabel='ylabel', title='title')

#changing ylables ticks
plt.yticks(np.arange(min(unique_pct), max(unique_pct)+1, 10.0))
y_value=['{:,.0f}'.format(x) + '%' for x in ax.get_yticks()]
ax.set_yticklabels(y_value)

plt.ylabel("Completeness %")
plt.xlabel("Types of galaxies")
plt.title("")
plt.show()
```

As we can see from the histograms @fig-dis-comp and @fig-type-comp the sample of unique galaxies of each catalog, gets smaller by an almost constant proportion (Inner join).

This means there is no bias in the selection of the galaxies.

# How are we going to compare the data?

## Scatter plots and $R^2$ calculation

1.  $R^2$: Measures the proportion of variance explained by the linear model.
2.  Slope of the Fitted Line: Should be close to 1 for a 1-1 correlation.[^1]

[^1]: Some data seem to have a very good linear correlation but they have many outliers. This is why we will clip the outliers with $\sigma > 3$

<!-- -->

3.  Pearson Correlation $\rho$: Measures the strength and direction of the linear relationship between two variables, ranging from -1 to 1. [^2]

[^2]: In simple linear regression, $R^2$ is the square of the Pearson correlation coefficient $\rho$.

```{python}
def compare_data(x, y, sigma = False):
    """
    Performs a linear comparison between two datasets.
    
    This function fits a linear model to the data, calculates the slope and intercept of the fitted line,
    and computes the R-squared value and Pearson correlation coefficient to assess the fit quality.
    
    Parameters:
    - x (array-like): The independent variable data.
    - y (array-like): The dependent variable data.
    - unc (array-like with units, optional): The uncertainties associated with the a variable data. Default is None.

    Returns:
    tuple: A tuple containing the following elements:
        - slope (float): The slope of the fitted linear model.
        - intercept (float): The intercept of the fitted linear model.
        - r2 (float): The R-squared value, indicating the proportion of variance explained by the linear model.
        - corr (float): The Pearson correlation coefficient, measuring the linear correlation between x and y.
    """
    try:
        x_data = np.ma.array(x.value, mask=np.isnan(x.value))
        y_data = np.ma.array(y.value, mask=np.isnan(y.value))
        # initialize a linear fitter
        fit = fitting.LinearLSQFitter()

        # initialize a linear model
        line_init = models.Linear1D()

        # fit the data with the fitter
        # check if sigma
        if sigma is False:
            fitted_line = fit(line_init, x_data, y_data)
            outliers = None
        else:
            or_fit = fitting.FittingWithOutlierRemoval(fit, sigma_clip, niter =3, sigma=3)
            fitted_line, outliers = or_fit(line_init, x_data, y_data)
        slope = fitted_line.slope.value
        intercept = fitted_line.intercept.value

        # Predict values using the fitted model
        y_pred = fitted_line(x_data)

        # Remove NaN values
        mask = ~np.isnan(y_data)
        if outliers is not None:
            mask = ~np.isnan(y_data) & ~outliers
        y_data_clean = y_data[mask]
        y_pred_clean = y_pred[mask]

        # Calculate R-squared
        r2 = r2_score(y_data_clean, y_pred_clean)

        # Calculate Pearson correlation coefficient
        corr = np.sqrt(np.abs(r2))

        return slope, intercept, r2, corr, outliers
    except Exception:
        return 0,0,0,0,None
```

4.  [Plots]{.underline}: Plots are essential for visually assessing the relationship between two datasets, identifying correlations, trends, and outliers, and evaluating the fit of linear models.

-   Histograms: Because not all of our data have the same number of counts, the comparison with histograms between data that are not the same, doesn't help us right now.[^3] This is why we will only use histograms for comparing the distribution of same-data columns normalized by their maximum value

[^3]: When we will use the outer join table we could use histograms due to the large number of counts.

<!-- -->

-   Correlation Heatmaps: A correlation heatmap is a graphical tool that displays the correlation between multiple variables as a color-coded matrix. It's like a color chart that shows us how closely related different variables are. In a correlation heatmap, each variable is represented by a row and a column, and the cells show the correlation between them. The color of each cell represents the strength and direction of the correlation, with darker colors indicating stronger correlations.

```{python}

# Define a function that creates a pairplot with correlation coefficients
def pairplot_with_correlation(df, cmap="coolwarm", font_size=12, font_style='italic', font_weight='bold', box_color='white'):
    """
    Creates a Seaborn pairplot with correlation coefficients displayed on the upper triangle.
    
    Parameters:
    df : pandas.DataFrame
        The dataframe containing the data for the pairplot.
    cmap : str, optional
        The colormap to use for coloring the correlation coefficients (default is 'coolwarm_r').
    font_size : int, optional
        The font size for the correlation coefficient annotations (default is 12).
    font_style : str, optional
        The font style for the correlation coefficient annotations (default is 'italic').
    font_weight : str, optional
        The font weight for the correlation coefficient annotations (default is 'bold').
    box_color : str, optional
        The background color of the box around the correlation coefficients (default is 'white').
    
    Returns:
    A Seaborn pairplot with correlation coefficients displayed in a heatmap style.
    """
    plt.close()
    # Create Pairplot
    pairplot = sns.pairplot(df, plot_kws={"color": "green"}, diag_kws = {"color" : "green"})

    # Calculate Correlations
    corr_matrix = df.corr()

    # Define a colormap
    colormap = plt.get_cmap(cmap)

    # Overlay Correlation Coefficients with customized annotations
    for i, j in zip(*np.triu_indices_from(corr_matrix, 1)):
        # Get the correlation value
        corr_value = corr_matrix.iloc[i, j]

        # Get a color from the colormap based on the correlation value
        color = colormap((corr_value + 1) / 2)  # Normalize between 0 and 1

        # Annotate the plot with the correlation value, using a box, bold, and italic
        pairplot.axes[i, j].annotate(f"{corr_value:.2f}", 
                                     xy=(0.5, 0.5), 
                                     xycoords='axes fraction',
                                     ha='center', 
                                     va='center', 
                                     fontsize=font_size,  # Font size
                                     fontstyle=font_style,  # Font style
                                     fontweight=font_weight,  # Font weight
                                     bbox=dict(facecolor=box_color, edgecolor=color, boxstyle='round,pad=0.5'),  # Box around the text
                                     color=color)  # Use heatmap color for text

    # Show the plot
    plt.show()

```

-   Kernel Density Estimate (KDE) plot: The KDE plot visually represents the distribution of data, providing insights into its shape, central tendency, and spread.

5.  Percentage change: We can calculate the percentage change of the data for each galaxy and then we can see if the data are similar, based on minimum, the maximum and the mean value of the difference.

$$\text{Percentage change} = \frac{V_{Hecate} - V_{LCV}}{V_{Hecate}}\cdot 100 \%$$

```{python}
def relative_diff(hec, lcv):
    """
    Calculate the relative difference between two sets of values.

    Parameters:
    hec (numpy.ndarray): An array of values representing the HEC dataset.
    lcv (numpy.ndarray): An array of values representing the LCV dataset.

    Returns:
    numpy.ndarray: An array of relative differences between the HEC and LCV datasets.
    The relative difference is calculated as ((HEC - LCV) * 100) / |HEC|.
    Any infinite values in the result are replaced with NaN.
    """
    diff = (hec - lcv) * 100 / np.abs(hec)
    diff[np.isinf(diff)] = np.nan
    return diff
```

# Comparable data

## Coordinates

|  LCV  | HECATE |   Description   | Pearson Correlation \[-1,1\] |
|:-----:|:------:|:---------------:|:----------------------------:|
|  Dis  |   D    |    Distance     |      `{python} round(compare_data(dt["Dis"],dt["D"])[3],3)`       |


```{python}
#| label: fig-coord-compare 
#| layout-align: center
#| fig-cap: "Comparison of the Distances"

plt.close()
fig, ax = plt.subplots()
sns.regplot(x = dt["Dis"].data, y = dt["D"].data,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))
ax.errorbar(dt["Dis"].data, dt["D"].data, yerr = dt["E_D"].data, fmt='none', capsize=2, zorder=1, color = "blue", alpha = 0.3)
results = scipy.stats.linregress(x = dt["Dis"], y=dt["D"])
plt.xlabel("$D_{LCV}$"+" [{}]".format(dt["Dis"].unit))
plt.ylabel("$D_{HEC}$"+" [{}]".format(dt["D"].unit))
plt.legend(["Data",
            "$D_{HEC}=$" +f"{results.slope:.2f}"+"$\cdot D_{LCV}+$"+
                f"$({results.intercept:+.2f})$"+
            f"\n $R^2 = {results.rvalue**2*100:.0f}\%$"])
plt.tight_layout()
plt.show()
```

- The average error of the distance in the HECATE catalog is $\overline{E_D} = \pm 1.6$ Mpc, so the intercept is included in the error.
- So we can assume that the Distances are the same

## Velocities

```{python}
rvel_corr = round(compare_data(dt["RVel"],dt["V"])[3],3)
```

|     LCV     |    HECATE    |              Description               |  Linear Correlation  |
|:----------------:|:-----------------:|:----------------:|:----------------:|
| RVel (km/s) |   V (km/s)   |      Heliocentric radial velocity      | `{python} rvel_corr` |
| VLG (km/s)  |              |            Radial velocity             |                      |
|  cz (km/s)  |              |         Heliocentric velocity          |                      |
|             | V_VIR (km/s) | Virgo-infall corrected radial velocity |                      |


```{python}
#| label: fig-vel-compare
#| layout-align: center
#| fig-cap: "Comparison of the Radial Velocities"
plt.close()
fig, ax = plt.subplots()
sns.regplot(x = dt["RVel"].data, y = dt["V"].data,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))

ax.errorbar(dt["RVel"].data, dt["V"].data, yerr = dt["E_V"].data, fmt='none', capsize=2, zorder=1, color = "blue")
results = scipy.stats.linregress(x = dt["RVel"], y=dt["V"])
plt.xlabel("$V_{LCV}$"+" [{}]".format(dt["RVel"].unit))
plt.ylabel("$V_{HEC}$"+" [{}]".format(dt["V"].unit))
plt.legend(["Data",
            "$V_{HEC}=$" +f"{results.slope:.2f}"+"$\cdot V_{LCV}+$"+
            f"$({results.intercept:+.2f})$"+
            f"\n $R^2 = {results.rvalue**2*100:.0f}\%$"])
plt.show()
```

- The average error of the radial velocity in the HECATE catalog is $\overline{E_V} = \pm 12\text{km}\cdot s^{-1}$ , so the intercept is included in the error.
- So we can assume that the radial velocities are the same

```{python}
vel_data = dt[["RVel", "V", "VLG", "cz", "V_VIR"]].to_pandas()
```

```{python}
pairplot_with_correlation(vel_data)
```

\[?\] The close correlation between all of the velocities, could be due to the fact that all of them measure the velocity of each galaxy, but from a different frame of reference.

## Morphology and Geometry

```{python}
dt["INCL"].mask = np.isnan(dt["INCL"])
```

```{python}
#| echo: false
 
ttype_corr = round(compare_data(dt["TType"], dt["T"])[3], 4)
inc_corr = round(compare_data(dt["inc"], dt["INCL"], sigma = True)[3], 3)
a26_corr = round(compare_data(dt["a26_1"], dt["R1"], sigma =False)[3], 3)
```

|      LCV      |        HECATE        |                        Description                        | Pearson Correlation \[-1,1\] |
|:----------------:|:----------------:|:-----------------:|:----------------:|
|     TType     |   T (with errors)    | Numerical Hubble type following the de Vaucouleurs system |    `{python} ttype_corr`     |
|      inc      |         INCL         |                     Inclination (deg)                     |     `{python} inc_corr`      |
| a26_1 (Major) | R1 (Semi-major axis) |                 angular diameter (arcmin)                 |     `{python} a26_corr`      |

::: panel-tabset
### Galaxy Types

```{python}
#| label: fig-types-compare
#| layout-ncol: 2
#| layout-align: center
#| fig-cap: "Comparison of the Types of galaxies"
#| fig-subcap: 
#| - "All the galaxies, without the correction" 
#| - "Comparison with the correction $|{T_{HECATE}-T_{LCV}}|<7$"

plt.close()
fig, ax = plt.subplots()
sns.regplot(x = dt["TType"].data, y = dt["T"].data,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))

ax.errorbar(dt["TType"].data, dt["T"].data, yerr = dt["E_T"].data, fmt='none', capsize=2, zorder=1, color = "blue")
temp = dt[["TType", "T"]].to_pandas().dropna()
results = scipy.stats.linregress(x = temp["TType"],
                                y = temp["T"])
plt.xlabel("$Type_{LCV}$")
plt.ylabel("$Type_{HEC}$")
plt.legend(["Data",
            "$T_{HEC}=$" +f"{results.slope:.2f}"+"$\cdot T_{LCV}+$"+
            f"$({results.intercept:+.2f})$"+
            f"\n $R^2 = {results.rvalue**2*100:.0f}\%$"])
plt.show()

types = dt[["T", "TType","E_T"]]
types["diff"] = dt["T"]-dt["TType"]
types = types[np.abs(types["diff"])<7]


plt.close()
fig, ax = plt.subplots()
sns.regplot(x = types["TType"].data, y = types["T"].data,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))

ax.errorbar(types["TType"].data, types["T"].data, yerr = types["E_T"].data, fmt='none', capsize=2, zorder=1, color = "blue")
results = scipy.stats.linregress(x= types["TType"].data,
                                y = types["T"].data)
plt.xlabel("$Type_{LCV}$")
plt.ylabel("$Type_{HEC}$")
plt.legend(["Data",
            "$T_{HEC}=$" +f"{results.slope:.2f}"+"$\cdot T_{LCV}+$"+
            f"$({results.intercept:+.2f})$"+
            f"\n $R^2 = {results.rvalue**2*100:.0f}\%$"])
plt.show()
```

"Morphological type of galaxy in the numerical code according to the classification by de Vaucouleurs et al. (1991). It should be noted that about three quarters of objects in the LV are dwarf galaxies, which require a more detailed morphological classification. For example, dwarf spheroidal galaxies and normal ellipticals are usually denoted by the same numerical code T \< 0, although their physical properties drastically differ. The classification problem arises as well for the “transient” type dwarf galaxies, T r, which combine the features of spheroidal (Sph) and irregular (Ir)systems. Due to small classification errors, such objects may “jump” from one end of the T scale to the other." @karachentsevUPDATEDNEARBYGALAXY2013

-   we can assume that the galaxies from the upper left and lower right regions of the plot are classified differently because of this
-   this might explain the large errors E_T of HECATE
-   why we see a difference in the distribution of @fig-type-comp 

We can remove the "problematic" galaxies by only keeping the ones with: 
$$
|{T_{HECATE}-T_{LCV}}|<7
$$


- The average uncertainty of the morphological type in the HECATE catalog is $\overline{E_T} = \pm 1.4$, so the intercept is included in the error.
- So we can assume that the Distances are the same


<!-- #### Normalize the scale of galaxy types

It is very possible that the two catalogs use different scaling methods, as indicated by the use of decimal numbers in HECATE.

```{python}
types = dt[["T", "TType"]].to_pandas()
```

```{python}
def min_max_normalize(series):
    return (series - series.min()) / (series.max() - series.min())

types["T_norm"] = min_max_normalize(types["T"])
types["T_norm"].mask = types["T"].mask
types["TType_norm"] = min_max_normalize(types["TType"])
#types.describe()
```
TType

Also, as we can see the minimum values are lower by 2 in HECATE, which complies with the linear fit.

```{python}
sns.histplot(types[["TType_norm", "T_norm"]], kde = True)
plt.xlabel("Types of galaxies")
plt.show()
```

```{python}
types["norm_diff"] = (-types["TType_norm"] + types["T_norm"])
sns.histplot(types["norm_diff"], kde = True)

plt.xlabel(r"$T_{HECATE}-T_{LCV}$")
plt.show()
```

```{python}
types["norm_rel_diff"] = (-types["TType_norm"] + types["T_norm"])/types["T"]
sns.histplot(types["norm_rel_diff"], kde = True)
plt.xlim(-0.2,0.2)
plt.xlabel(r"$(T_{HECATE}-T_{LCV})/T_{HECATE}$")
plt.show()
```
-->


### Inclination

```{python}
#| layout: [[50,50]]
plt.close()
temp = dt[["inc", "INCL"]].to_pandas()

sns.histplot(temp["INCL"], kde = True, label = "HECATE")
sns.histplot(temp["inc"], kde = True, label = "LCV")
plt.legend()
plt.xlabel("Inclination of galaxies [deg]")
plt.show()

temp["Percentage Change [%]"] = (-temp["inc"] + temp["INCL"])/temp["INCL"]
temp.loc[np.isinf(temp["Percentage Change [%]"]), "Percentage Change [%]"] = np.nan
sns.histplot(temp["Percentage Change [%]"], kde = True)
plt.show()
```
```{python}
temp.describe()
```

We can see that for values in the range $[\sim 30^\circ,\sim 80^\circ]$, the values of the LCV inclination are higher. However, since their means, median, min and maxes are similar and the percentage change is practically 0% (mean, median, $\sigma$ = 0 with a range $[-3\%,1\%]$), we can ignore the differences and assume they are the same values.

### Major Axis

```{python}
#| label: fig-axis-compare
#| layout-ncol: 2
#| layout-align: center
#| fig-cap: "Comparison of the Major Axises of the galaxies"
#| fig-subcap: 
#| - "Linear scale" 
#| - "$log_{10}$ scale"
plt.close()
fig, ax = plt.subplots()
sns.regplot(x = dt["a26_1"].data, y = dt["R1"].data,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))
temp = dt[["a26_1","R1"]].to_pandas().dropna()
results = scipy.stats.linregress(x = temp["a26_1"], y = temp["R1"])

plt.xlabel("(Major angular diameter$)_{LCV}$")
plt.ylabel("(Semi major axis$)_{HEC}$")
plt.legend(["Data",
            "$R1_{HEC}=$" +f"{results.slope:.2f}"+"$\cdot a_{26,LCV}$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.2f}\%$"])
plt.show()
dt["log_a26"] = np.log10(dt["a26_1"].data)
dt["log_R1"] = np.log10(dt["R1"].data)
plt.close()
fig, ax = plt.subplots()
p = sns.regplot(x = dt["log_a26"].data, y = dt["log_R1"].data,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))

temp = dt[["log_a26","log_R1"]].to_pandas().dropna()
results = scipy.stats.linregress(x = temp["log_a26"], y = temp["log_R1"])
plt.xlabel("(Major angular diameter$)_{LCV}$"+" [{}]".format(dt["a26_1"].unit))
plt.ylabel("(Semi major axis$)_{HEC}$"+" [{}]".format(dt["R1"].unit))
plt.legend(["Data",
            "$log_{10}(R1_{HEC})=$" +f"{results.slope:.2f}"+"$\cdot log_{10}(a_{26,LCV})$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.0f}\%$"])
plt.show()
```

it is not very clear if we truly have a correlation or not. We need to see the linear correlation of the decimal logarithms.

$\overline{R_1} = 3.9$ [arcmin],  $\overline{a_{26}} = 9.3$ [arcmin], so the intercept is negligable.
$$
R_1 = 0.48\cdot a_{26}-0.34 \sim \frac{1}{2}a_{26}
$$

$$
\log(R1) = 0.89\log(a_{26})-10^{0.38} = \log(10^{-0.38}a_{26}^{0.89}) = \log(0.41\cdot a_{26}^{0.89})\Rightarrow R1\simeq \frac{a_{26}}{2}
$$

:::

## Luminosities

```{python}
#| echo: false
#| warning: false
logKLum_corr = round(compare_data(dt["logKLum"], dt["logL_K"], sigma = True)[3], 3)

```

|   LCV   | HECATE | Description | Pearson Correlation \[-1,1\] |
|:-------:|:------:|:-----------:|:----------------------------:|
| logKLum | logL_K |             |   `{python} logKLum_corr`    |

```{python}
#| label: fig-KLum-compare
#| fig-cap: "Comparison of the $L_K$ of the galaxies"
plt.close()
fig, ax = plt.subplots()
sns.regplot(x = dt["logKLum"], y = dt["logL_K"],
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))
temp = dt[["logKLum","logL_K"]].to_pandas().dropna()
results = scipy.stats.linregress(x = temp["logKLum"], y = temp["logL_K"])
plt.xlabel("log($L_{K, LCV} $"+"/{})".format(dt["KLum"].unit.to_string("latex")))
plt.ylabel("log($L_{K, HECATE} $"+"/{})".format(dt["KLum"].unit.to_string("latex")))
plt.legend(["Data",
            "$log(L_{K,HEC})=$" +f"{results.slope:.2f}"+"$\cdot log(L_{K,LCV})$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.2f}\%$"])
plt.show()
```

$$
\log(L_{K,HEC})=1.16\log(L_{K,LCV})-1.78=\log\left(\frac{{L_{K,LCV}}^{1.16}}{10^{1.78}}\right) \Leftrightarrow L_{K,HEC}=0.02\cdot{L_{K,LCV}}^{1.16}
$$

So as we can see the usage of a linear fitting is not correct here.
But we can clearly see a linear correlation.

We will use the relative difference of the two luminosities, to see if statistically they are the same:

```{python}
#| echo: false
dt["diff_L_K"] = relative_diff(hec = dt["logL_K"].value, lcv = dt["logKLum"])
temp = dt[["logKLum", "logL_K", "diff_L_K"]].to_pandas()
sns.histplot(temp["diff_L_K"], kde = True)
plt.xlabel("Percentage change [%]")
plt.title("K-band Luminosities")
plt.show()
```

```{python}
temp.rename(columns = {"logKLum":"log(L_K)_{LCV}$", "logL_K":"log(L_K)_{HEC}", "diff_L_K":"Percentage Change [%]"}, inplace=True)
```

```{python}
temp.describe(percentiles=[])
```

## Magnitudes

```{python}
#| output: false 
#| warning: false
mag_B_corr = round(compare_data(dt["mag_B"], dt["BT"], sigma = True)[3], 3)
Kmag_corr = round(compare_data(dt["Kmag"], dt["K"], sigma = True)[3], 3)
```

|         LCV         |      HECATE      |         Description         | Pearson Correlation \[-1,1\] |
|:----------------:|:----------------:|:----------------:|:-----------------:|
| mag_B (with errors) | BT (with errors) |                             |    `{python} mag_B_corr`     |
|        Kmag         |        K         | 2MASS band magnitude (both) |     `{python} Kmag_corr`     |

```{python}
#| label: fig-Mag-compare
#| layout-ncol: 2
#| layout-align: center
#| fig-cap: "Comparison of the Magnitudes of the galaxies"
#| fig-subcap: 
#| - "$M_B$" 
#| - "$M_K$"
#| 
plt.close()
fig, ax = plt.subplots()
sns.regplot(x = dt["mag_B"].data, y = dt["BT"].data,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))
ax.errorbar(dt["mag_B"].data, dt["BT"].data, xerr = dt["e_mag_B"].data, yerr = dt["E_BT"].data, fmt='none', capsize=2, zorder=1, color = "blue")
temp = dt[["mag_B","BT"]].to_pandas().dropna()
results = scipy.stats.linregress(x = temp["mag_B"], y = temp["BT"])
plt.xlabel("$M_{B,LCV}$"+" [{}]".format(dt["mag_B"].unit.to_string("latex")))
plt.ylabel("$M_{B,HECATE} $"+" [{}]".format(dt["BT"].unit.to_string("latex")))
plt.legend(["Data",
            "$M_{B,HEC}=$" +f"{results.slope:.2f}"+"$\cdot M_{B,LCV}$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.2f}\%$"])
plt.show()

#M_K
plt.close()
fig, ax = plt.subplots()
sns.regplot(x = dt["Kmag"].data, y = dt["K"].data,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))
ax.errorbar(dt["Kmag"].data, dt["K"].data, yerr = dt["E_K"].data, fmt='none', capsize=2, zorder=1, color = "blue")
temp = dt[["Kmag","K"]].to_pandas().dropna()
results = scipy.stats.linregress(x = temp["Kmag"], y = temp["K"])
plt.xlabel("$M_{K,LCV}$"+" [{}]".format(dt["Kmag"].unit.to_string("latex")))
plt.ylabel("$M_{K,HECATE} $"+" [{}]".format(dt["K"].unit.to_string("latex")))
plt.legend(["Data",
            "$M_{K,HEC}=$" +f"{results.slope:.2f}"+"$\cdot M_{K,LCV}$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.2f}\%$"])
plt.show()
```

- $M_B$: it is a 1-1 correlation, since the average error $M_{B,HECATE} = 0.4\ mag$, so the itercept is smaller than the error
- $M_K$: we need to examine it more, since the intercept is bigger than the error $M_{K,HECATE} = 0.09\ mag$

```{python}
dt["diff_K_mag"] = relative_diff(dt["Kmag"], dt["K"])

temp = dt[["diff_K_mag"]].to_pandas()
c = temp["diff_K_mag"]
up_clip = c.mean() + 3*c.std()
low_clip = c.mean() - 3*c.std()
temp["Percentage Change, after 3 sigma clip [%]"] = temp["diff_K_mag"].clip(low_clip, up_clip)
temp.rename(columns={"diff_K_mag":"Percentage Change [%]"}, inplace=True)
```

```{python}
sns.histplot(temp, kde = True)
plt.title("Percentage change Distribution [%]")
plt.xlabel(f"K band magnitude [{dt['K'].unit}]")
plt.show()

sns.histplot(dt["Kmag"].data, label = "LCV")
sns.histplot(dt["K"].data, label = "HECATE")
plt.legend()
plt.xlabel(f"K band magnitude [{dt['K'].unit}]")
plt.show()
```
```{python}
temp.describe(percentiles=[])
```

\[?\] this 5\% average in the difference probably exists because of the low number of galaxies we compare, so we can ignore it.

## SFR

```{python}
#| echo: false
# Extract the relevant SFR columns
sfr_columns = ["logSFR_TIR", "logSFR_FIR", "logSFR_60u", "logSFR_12u", "logSFR_22u", "logSFR_HEC", "logSFR_GSW", "logSFRFUV", "logSFRHa"]
sfr_data = dt[sfr_columns].to_pandas()

# Count the number of non-NaN cells for each column
non_nan_counts = sfr_data.notna().sum()
```

|  LCV   |   HECATE   |                           Description                            |                  Count                  |
|:----------------:|:----------------:|:-----------------:|:----------------:|
|        | logSFR_TIR | Decimal logarithm of the total-infrared SFR estimate \[Msol/yr\] | `{python} non_nan_counts["logSFR_TIR"]` |
|        | logSFR_FIR |  Decimal logarithm of the far-infrared SFR estimate \[Msol/yr\]  | `{python} non_nan_counts["logSFR_FIR"]` |
|        | logSFR_60u |      Decimal logarithm of the 60um SFR estimate \[Msol/yr\]      | `{python} non_nan_counts["logSFR_60u"]` |
|        | logSFR_12u |      Decimal logarithm of the 12um SFR estimate \[Msol/yr\]      | `{python} non_nan_counts["logSFR_12u"]` |
|        | logSFR_22u |      Decimal logarithm of the 22um SFR estimate \[Msol/yr\]      | `{python} non_nan_counts["logSFR_22u"]` |
|        | logSFR_HEC |  Decimal logarithm of the homogenised SFR estimate \[Msol/yr\]   | `{python} non_nan_counts["logSFR_HEC"]` |
|        | logSFR_GSW |       Decimal logarithm of the SFR in GSWLC-2 \[Msol/yr\]        | `{python} non_nan_counts["logSFR_GSW"]` |
| SFRFUV |            |             FUV derived integral star formation rate             | `{python} non_nan_counts["logSFRFUV"]`  |
| SFRHa  |            |          H{alpha} derived integral star formation rate           |  `{python} non_nan_counts["logSFRHa"]`  |

```{python}
#| echo: false
sfr_data.drop("logSFR_GSW", axis=1, inplace=True)

# Example usage of the function
pairplot_with_correlation(sfr_data)

```

```{python}
#| label: fig-sfr-lcv
#| layout-ncol: 2
#| layout-align: center
#| fig-cap: "Comparison of the $SFR_{FUV}-SFR_{Ha} of the galaxies"
#| fig-subcap: 
#| - "Linear scale" 
#| - "Decimal logarithmic scale"

##linear
plt.close()
fig, ax = plt.subplots()
temp = lcv[["SFRHa","SFRFUV"]].to_pandas().dropna()
x = temp["SFRHa"]
y = temp["SFRFUV"]
sns.regplot(x = x, y = y,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))
results = scipy.stats.linregress(x = x, y = y)
plt.axvline(x=1e-3, color='green', linestyle='--', linewidth=2)
plt.xlabel("$SFR_{Ha}$"+" [{}]".format(dt["SFRHa"].unit.to_string("latex")))
plt.ylabel("$SFR_{FUV} $"+" [{}]".format(dt["SFRFUV"].unit.to_string("latex")))
plt.legend(["Data",
            "$SFR_{FUV}=$" +f"{results.slope:.2f}"+"$\cdot SFR_{Ha}$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.2f}\%$"])
plt.show()

##log

plt.close()

x = np.log10(temp["SFRHa"])
y = np.log10(temp["SFRFUV"])
fig, ax = plt.subplots()
sns.regplot(x = x, y = y,
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))
results = scipy.stats.linregress(x = x, y = y)
plt.axvline(x=-3, color='green', linestyle='--', linewidth=2)

plt.xlabel("$log(SFR_{Ha})$"+" [{}]".format(dt["logSFR_HEC"].unit.to_string("latex")))
plt.ylabel("$log(SFR_{FUV})$"+" [{}]".format(dt["logSFR_HEC"].unit.to_string("latex")))
plt.legend(["Data",
            "$log(SFR_{FUV})=$" +f"{results.slope:.2f}"+"$\cdot log(SFR_{Ha})$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.2f}\%$"])
plt.show()
```

The SFR according to @kroupaConstraintsStarFormation2020, can be calculated from the mean of SFR from the Ha and FUV, for $SFR>10^{-3}\ M_\odot\, yr^{-1}$. As we can see from the plots @fig-sfr-lcv it is a good aproximation for all SFR's

$$
SFR = \frac{SFR_{FUV}+SFR_{H\alpha}}{2}, \text{if both exist, else: } SFR = SFR_i,\ i= FUV,\, H\alpha
$$

```{python}
sfr_fuv = dt['SFRFUV']
sfr_ha = dt['SFRHa']

dt["SFR_t"] =np.log10(np.nanmean([sfr_fuv, sfr_ha], axis=0))
dt["SFR_t"].unit = dt["SFRFUV"].unit
```

```{python}
#| label: fig-SFR-compare
#| layout-ncol: 2
#| layout-align: center
#| fig-cap: "Comparison of the SFR's of the galaxies"
#| fig-subcap: 
#| - "Linear scale" 
#| - "Decimal logarithmic scale"

##linear
plt.close()
fig, ax = plt.subplots()
temp = dt[["SFR_t","logSFR_HEC"]].to_pandas().dropna()
temp = temp[temp["SFR_t"]>-3]
sns.regplot(x = 10**temp["SFR_t"], y = 10**temp["logSFR_HEC"],
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))
results = scipy.stats.linregress(x = 10**temp["SFR_t"], y = 10**temp["logSFR_HEC"])
plt.xlabel("$SFR_{LCV}$"+" [{}]".format(dt["SFR_t"].unit.to_string("latex")))
plt.ylabel("$SFR_{HECATE} $"+" [{}]".format(dt["SFR_t"].unit.to_string("latex")))
plt.legend(["Data",
            "$SFR_{HEC}=$" +f"{results.slope:.2f}"+"$\cdot SFR_{LCV}$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.2f}\%$"])
plt.show()

##log

plt.close()
fig, ax = plt.subplots()
temp = dt[["SFR_t","logSFR_HEC"]].to_pandas().dropna()
temp = temp[temp["SFR_t"]>-3]
sns.regplot(x = temp["SFR_t"], y = temp["logSFR_HEC"],
                 scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'),
                 line_kws=dict(alpha=0.7, color='red', linewidth=3))
results = scipy.stats.linregress(x = temp["SFR_t"], y = temp["logSFR_HEC"])
plt.xlabel("$log(SFR_{LCV})$"+" [{}]".format(dt["logSFR_HEC"].unit.to_string("latex")))
plt.ylabel("$log(SFR_{HECATE})$"+" [{}]".format(dt["logSFR_HEC"].unit.to_string("latex")))
plt.legend(["Data",
            "$log(SFR_{HEC})=$" +f"{results.slope:.2f}"+"$\cdot log(SFR_{LCV})$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.2f}\%$"])
plt.show()
```

?EXPLANATION? this low correlation maybe comes from the inaccuracy/approximation of @kroupaConstraintsStarFormation2020

## Masses

```{python}
#| echo: false
# Extract the relevant mass columns
mass_columns = ["logM26", "logMHI", "logM_HEC", "logM_GSW", "logStellarMass"]
mass_data = dt[mass_columns].to_pandas()

# Count the number of non-NaN cells for each column
non_nan_counts = mass_data.notna().sum()
```

|      LCV       |  HECATE  |                        Description                        |                    Count                    |
|:----------------:|:----------------:|:----------------:|:----------------:|
|     logM26     |          |              Log mass within Holmberg radius              |     `{python} non_nan_counts["logM26"]`     |
|     logMHI     |          |              Log mass within Holmberg radius              |     `{python} non_nan_counts["logMHI"]`     |
|                | logM_HEC |      Decimal logarithm of the stellar mass \[Msol\]       |    `{python} non_nan_counts["logM_HEC"]`    |
|                | logM_GSW | Decimal logarithm of the stellar mass in GSWLC-2 \[Msol\] |    `{python} non_nan_counts["logM_GSW"]`    |
| logStellarMass |          |               Stellar Mass from $M_*/L=0.6$               | `{python} non_nan_counts["logStellarMass"]` |

::: panel-tabset

### Stellar Masses Comparison

```{python}
#| label: fig-Mass-compare
#| fig-cap: "Comparison of the Stellar Masses of the galaxies"

plt.close()
fig, ax = plt.subplots()
temp = dt[["logStellarMass","logM_HEC"]].to_pandas().dropna()
x = temp["logStellarMass"]
y = temp["logM_HEC"]
sns.regplot(x = x, y = y, 
                scatter_kws=dict(alpha=0.5, color='blue', edgecolors='white'), 
                line_kws=dict(alpha=0.7, color='red', linewidth=3))
results = scipy.stats.linregress(x = x, y = y)
plt.xlabel("$log(M_{*,LCV})$"+" [{}]".format(dt["logM_HEC"].unit.to_string("latex")))
plt.ylabel("$log(M_{*,HECATE} )$"+" [{}]".format(dt["logM_HEC"].unit.to_string("latex")))
plt.legend(["Data",
            "$log(M_{*,HEC})=$" +f"{results.slope:.2f}"+"$\cdot log(M_{*,LCV})$"+
            f"${results.intercept:+.2f}$"+
            f"\n $R^2 = {results.rvalue**2*100:.2f}\%$"])
plt.show()

```
```{python}

dt["diff_M"] =  relative_diff(hec = dt["logM_HEC"].data, lcv = dt["logStellarMass"])
temp = dt[["diff_M"]].to_pandas()
sns.histplot(temp, kde = True)
plt.title("Percentage change Distribution [%]")
plt.xlabel(r"$\log_{10}M_*$ "+f"[{dt['logM_HEC'].unit}]")
plt.show()
plt.close()

```

$$
\log{M_{*,HECATE}}=1.07\cdot\log{M_{*,LCV}}-0.69, \text{ but } \log{M_*}>6\Rightarrow M_{*,HEC}\sim M_{*,LCV}
$$

As we can see the aproximation of Mass/Light=const.=0.6 is a pretty good approximation, for the calculation of $\log(M_*/M_\odot)$, especially for high-mass galaxies

### Heatmap

```{python}
mass_data.drop("logM_GSW", axis=1, inplace=True)
# Compute the correlation matrix

pairplot_with_correlation(mass_data)
```
:::